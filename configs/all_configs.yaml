# llm_configs.yaml
llm_configs:
  type: ollama
  base_url: "http://localhost:11434"
  model: "llama3:8b" # General model
  temperature: 0.2
  timeout: 60
  
  # Role-specific model overrides (optional)
  roles:
    admin:
      model: "llama3:70b"  # Heavier model for complex admin tasks
    support:
      model: "llama3:8b"
      